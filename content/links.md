---
layout: default
title: Links
---

## Papers

- [An Approach Based on Bayesian Networks for Query Selectivity Estimation - DASFAA, 2019](https://link.springer.com/chapter/10.1007/978-3-030-18579-4_1)
- Master 2 year internship at [HelloFresh](https://www.hellofresh.com/) ([report](/files/internships/M2_report.pdf), [slides](/files/internships/M2_slides.pdf))
- Master 1 year internship at [Privateaser](https://www.privateaser.com/) ([report](/files/internships/M1_report.pdf), [slides](/files/internships/M1_slides.pdf))
- Undergraduate internship at [INSA Toulouse](http://www.insa-toulouse.fr/fr/index.html) ([report](/files/internships/L3_report.pdf), [slides](/files/internships/L3_slides.pdf))
- [Detailed solutions to the first 30 Project Euler problems](/files/papers/project_euler_solutions.pdf)

## Talks

I've given a few talks at conferences and companies.

- [The Benefits of Online Learning - Quantmetry, Paris, 2019](/slides/the-benefits-of-online-learning)
- [The Benefits of Online Learning - Element AI, London, 2019](/slides/the-benefits-of-online-learning)
- [The Benefits of Online Learning - Airbus BizLab, Toulouse, 2019](/slides/the-benefits-of-online-learning)
- [An Approach Based on Bayesian Networks for Query Selectivity Estimation - DASFAA, 2019](/slides/dasfaa-2019.pdf)
- [Machine learning incrémental: des concepts à la pratique - Toulouse Data Science, 2019](/slides/creme-tds)
- [Online machine learning with creme - PyData, Amsterdam, 2019](/slides/creme-pydata)
- [Docker for data science - HelloFresh, Berlin, 2017](/slides/docker-data-science.pdf)
- [Challenge Big Data - Toulouse, 2017](https://www.youtube.com/watch?v=oQd1h-8Srf4&feature=youtu.be)
- [Forecasting bicycle-sharing usage - Toulouse Data Science, 2016](https://www.youtube.com/watch?v=vQGdzKkyPP0)

## Datasets

- [OpenBikes 2016 challenge](https://www.dropbox.com/s/ic8m0b3mf5wxk4r/challenge.zip?dl=0)

## Hall of fame

The following is a hall of fame of papers, books, and blog posts that have a very high [signal to noise ratio](https://www.urbandictionary.com/define.php?term=signal%20to%20noise%20ratio); I highly recommend reading some of them when you get time.

- [The Elements of Statistical Learning - Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)
- [Machine Learning - Tom Mitchell](http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf) -- I think this wonderful textbook is under-appreciated.
- [Artificial Intelligence: A Modern Approach - Russel & Norvig](http://web.cecs.pdx.edu/~mperkows/CLASS_479/2017_ZZ_00/02__GOOD_Russel=Norvig=Artificial%20Intelligence%20A%20Modern%20Approach%20(3rd%20Edition).pdf)
- [mlcourse.ai](https://mlcourse.ai/) -- Of all the introductions to machine learning I think this is the one that strikes the best balance between theory and practice.
- [Machine learning cheat sheets - Shervine Amidi](https://stanford.edu/~shervine/teaching/cs-229.html)
- [Kalman and Bayesian Filters in Python - Roger Labbe](http://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb) -- Kalman filters are notoriously hard to grok, this tutorial nicely builds up the steps to understanding them.
- [CS231n Convolutional Neural Networks for Visual Recognition - Stanford](http://cs231n.github.io/convolutional-networks/)
- [Algorithmes d’optimisation non-linéaire sans contrainte (French) - Michel Bergmann](https://www.math.u-bordeaux.fr/~mbergman/PDF/These/annexeC.pdf)
- [Graphical Models in a Nutshell - Koller et al.](https://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf)
- [Rules of Machine Learning: Best Practices for ML Engineering - Martin Zinkevich](https://developers.google.com/machine-learning/guides/rules-of-ml/) -- You should read this once a year.
- [A Few Useful Things to Know about Machine Learning - Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) -- This short paper summarizes basic truths in machine learning.
- [Choose Boring Technology - Dan McKinley](http://mcfunley.com/choose-boring-technology)
- [How to Write a Spelling Corrector - Peter Norvig](https://norvig.com/spell-correct.html) -- Magic in 36 lines of code.
- [MCMC sampling for dummies - Thomas Wiecki](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)
- [Your Easy Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)
- [An Intuitive Explanation of Convolutional Neural Networks - Ujjwal Karn](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)
- [An overview of gradient descent optimization algorithms - Sebastian Ruder](http://ruder.io/optimizing-gradient-descent/)
- [How to explain gradient boosting - Terence Parr and Jeremy Howard](https://explained.ai/gradient-boosting/index.html) -- A very good introduction to vanilla gradient boosting with step by step examples.
- [Why Does XGBoost Win "Every" Machine Learning Competition? - Didrik Nielsen](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf) -- This Master's thesis goes into some of the details of XGBoost without being too bloated.
- [Good sleep, good learning, good life - Piotr Wozniak](https://web.archive.org/web/20181017190008/https://www.supermemo.com/en/articles/sleep) -- Extremely long and nothing to do with data science, but a very thorough essay nonetheless on how to properly sleep.
- [Make for data scientists - Paul Butler](http://blog.kaggle.com/2012/10/15/make-for-data-scientists/) -- I believe Makefiles are yet to be rediscovered for managing data science pipelines.
- [Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations](https://fermatslibrary.com/s/statistical-tests-p-values-confidence-intervals-and-power-a-guide-to-misinterpretations) -- Just read it.
- [The Cramér-Rao Lower Bound on Variance: Adam and Eve’s "Uncertainty Principle" - Michael Powers](https://web.archive.org/web/20100613220918/http://astro.temple.edu/~powersmr/vol7no3.pdf)
- [Kaggle contest on Observing Dark World - Cam Davidson-Pilon](https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter5_LossFunctions/Ch5_LossFunctions_PyMC3.ipynb#Example:-Kaggle-contest-on-Observing-Dark-World) -- If you're not convinced about the power of Bayesian machine learning then read this and get your mind blown.
- [A Concrete Introduction to Probability (using Python) - Peter Norvig](https://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb) -- Extremely elegant Python coding.
- [The Hungarian Maximum Likelihood Trick - Louis Abraham](https://louisabraham.github.io/notebooks/hungarian_trick.html)
- [Machine Learning for Signal Processing - University of Illinois](https://courses.engr.illinois.edu/cs598ps/fa2018/material.html)
- [Don't Call Yourself A Programmer, And Other Career Advice](https://www.kalzumeus.com/2011/10/28/dont-call-yourself-a-programmer/)
- [Tidy Data - Hadley Wickham](https://vita.had.co.nz/papers/tidy-data.pdf) -- If you like playing with data then you need to be aware of this one.
- [Gaussian Process, not quite for dummies - Yuge Shi](https://yugeten.github.io/posts/2019/09/GP/) -- Gaussian processes are quite difficult to understand (at least, for me) but Yuge gives some great visual intuitions.
- [Continuous Delivery for Machine Learning - Martin Fowler](https://martinfowler.com/articles/cd4ml.html)
- [Memos - Sriram Krishnan](https://sriramk.com/memos)
- [Frequentism and Bayesianism: A Python-driven Primer](https://arxiv.org/pdf/1411.5018.pdf)

## Blogs

This is a list of blogs I regularly scroll through.

- [Tim Salimans on Data Analysis](http://timsalimans.com/)
- [Randal Olson](http://www.randalolson.com/blog/)
- [Sam & Max](http://sametmax.com/) -- French and NSFW!
- [Sebastian Raschka](http://sebastianraschka.com/blog/index.html)
- [Clean Coder](https://sites.google.com/site/unclebobconsultingllc/)
- [Pythonic Perambulations](https://jakevdp.github.io/)
- [Erik Bernhardsson](http://erikbern.com/)
- [otoro](http://blog.otoro.net/)
- [Terra Incognita](http://blog.christianperone.com/)
- [Real Python](https://realpython.com/blog/)
- [Airbnb Engineering](http://nerds.airbnb.com/)
- [No Free Hunch](http://blog.kaggle.com/)
- [The Unofficial Google Data Science Blog](http://www.unofficialgoogledatascience.com/)
- [will wolf](http://willwolf.io/)
- [Edwin Chen](http://blog.echen.me/)
- [Use the index, Luke!](http://use-the-index-luke.com/)
- [Jack Preston](https://unwttng.com/)
- [Agustinus Kristiadi](https://wiseodd.github.io/)
- [DataGenetics](http://datagenetics.com/blog.html)
- [Katherine Bailey](https://katbailey.github.io/)
- [Netflix Research](https://research.netflix.com/)
- [inFERENce](https://www.inference.vc/)
- [Hyndsight](https://robjhyndman.com/hyndsight/) -- Rob Hyndman is a time series specialist
- [While My MCMC Gently Samples](https://twiecki.io/)
- [Ines Montani](https://ines.io/) -- By one of the founders of [spaCy](https://spacy.io/)
- [Stephen Smerity](https://smerity.com/articles/articles.html)
- [Peter Norvig](http://norvig.com/)
- [IT Best Kept Secret Is Optimization](https://www.ibm.com/developerworks/community/blogs/jfp/?lang=en) -- By Jean-Francois Puget, aka [CPMP](https://www.kaggle.com/cpmpml)
- [explained.ai](https://explained.ai/)
- [Better Explained](https://betterexplained.com/)
- [Genetic Argonaut](https://geneticargonaut.blogspot.com/)
- [pandas blog](https://pandas-dev.github.io/pandas-blog/)
- [Towards Data Science](https://towardsdatascience.com/)
- [Linear Disgressions](https://lineardigressions.com/) -- data science podcasts
- [Probably Overthinking It](https://www.allendowney.com/blog/)
- [Simply Statistics](https://simplystatistics.org/)
- [Practically Predictable](http://practicallypredictable.com/)
- [koaning](http://koaning.io/) -- By Vincent Warmerdam, who did [this](https://www.youtube.com/watch?v=68ABAU_V8qI&feature=youtu.be) great presentation
- [blogarithms](https://blogarithms.github.io/)
- [Possibly Wrong ](https://possiblywrong.wordpress.com/)
