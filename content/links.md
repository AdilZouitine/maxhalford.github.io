---
layout: default
title: Links
---

## Papers

- [Cost models in database query optimisation bibliography](https://www.sharelatex.com/project/59f0867a5cd281015b4cd5c6)
- [Detailed solutions to the first 30 Project Euler problems](/files/papers/project_euler_solutions.pdf)

## Presentations

- [Docker for data science](/files/presentations/docker-data-science.pdf)

## Datasets

- [OpenBikes 2016 challenge](https://www.dropbox.com/s/ic8m0b3mf5wxk4r/challenge.zip?dl=0)

## Internship hand ins

- Undergraduate ([report](/files/internships/L3_report.pdf), [slides](/files/internships/L3_slides.pdf))
- First Master's year ([report](/files/internships/M1_report.pdf), [slides](/files/internships/M1_slides.pdf))

## Hall of fame

The following is a hall of fame of papers, books, and blog posts that have a very high [signal to noise ratio](https://www.urbandictionary.com/define.php?term=signal%20to%20noise%20ratio) and that I thoroughly recommend.

- [The Elements of Statistical Learning - Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)
- [Machine Learning - Tom Mitchell](http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf) -- I think this wonderful textbook is under-appreciated.
- [Artificial Intelligence: A Modern Approach - Russel & Norvig](http://web.cecs.pdx.edu/~mperkows/CLASS_479/2017_ZZ_00/02__GOOD_Russel=Norvig=Artificial%20Intelligence%20A%20Modern%20Approach%20(3rd%20Edition).pdf)
- [mlcourse.ai](https://mlcourse.ai/) -- Of all the introductions to machine learning I think this is the one that strikes the best balance between theory and code.
- [Machine learning cheat sheets - Shervine Amidi](https://stanford.edu/~shervine/teaching/cs-229.html)
- [Kalman and Bayesian Filters in Python - Roger Labbe](http://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb) -- Kalman filters are notoriously hard to grok, this tutorial nicely builds up the steps to understanding them.
- [CS231n Convolutional Neural Networks for Visual Recognition - Stanford](http://cs231n.github.io/convolutional-networks/)
- [Algorithmes d’optimisation non-linéaire sans contrainte (French) - Michel Bergmann](https://www.math.u-bordeaux.fr/~mbergman/PDF/These/annexeC.pdf)
- [Graphical Models in a Nutshell - Koller et al.](https://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf)
- [Rules of Machine Learning: Best Practices for ML Engineering - Martin Zinkevich](https://developers.google.com/machine-learning/guides/rules-of-ml/) -- You should read this once a year.
- [A Few Useful Things to Know about Machine Learning - Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) -- This short paper summarizes basic truths in machine learning.
- [Choose Boring Technology - Dan McKinley](http://mcfunley.com/choose-boring-technology)
- [How to Write a Spelling Corrector - Peter Norvig](https://norvig.com/spell-correct.html) -- Magic in 36 lines of code.
- [MCMC sampling for dummies - Thomas Wiecki](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)
- [Your Easy Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)
- [An Intuitive Explanation of Convolutional Neural Networks - Ujjwal Karn](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)
- [An overview of gradient descent optimization algorithms - Sebastian Ruder](http://ruder.io/optimizing-gradient-descent/)
- [How to explain gradient boosting - Terence Parr and Jeremy Howard](https://explained.ai/gradient-boosting/index.html) -- A very good introduction to vanilla gradient boosting with step by step examples.
- [Why Does XGBoost Win "Every" Machine Learning Competition? - Didrik Nielsen](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf) -- This Master's thesis goes into some of the details of XGBoost without being too bloated.
- [Good sleep, good learning, good life - Piotr Wozniak](https://web.archive.org/web/20181017190008/https://www.supermemo.com/en/articles/sleep) -- Extremely long and nothing to do with data science, but a very thorough essay nonetheless on how to properly sleep.
- [Make for data scientists - Paul Butler](http://blog.kaggle.com/2012/10/15/make-for-data-scientists/) -- I believe Makefiles are yet to be rediscovered for managing data science pipelines.
- [Statistical tests, P values, conﬁdence intervals, and power: a guide to misinterpretations](https://fermatslibrary.com/s/statistical-tests-p-values-confidence-intervals-and-power-a-guide-to-misinterpretations#email-newsletter) -- Just read it.
