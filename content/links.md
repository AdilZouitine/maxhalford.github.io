---
layout: default
title: Links
---

## Papers

- [Cost models in database query optimisation bibliography](https://www.sharelatex.com/project/59f0867a5cd281015b4cd5c6)
- [Detailed solutions to the first 30 Project Euler problems](/files/papers/project_euler_solutions.pdf)

## Presentations

- [Docker for data science](/files/presentations/docker-data-science.pdf)

## Datasets

- [OpenBikes 2016 challenge](https://www.dropbox.com/s/ic8m0b3mf5wxk4r/challenge.zip?dl=0)

## Internship hand ins

- Undergraduate ([report](/files/internships/L3_report.pdf), [slides](/files/internships/L3_slides.pdf))
- First Master's year ([report](/files/internships/M1_report.pdf), [slides](/files/internships/M1_slides.pdf))

## Hall of fame

The following is a hall of fame of papers, books, and blog posts that have a very high [signal to noise ratio](https://www.urbandictionary.com/define.php?term=signal%20to%20noise%20ratio); I highly recommend reading some of them when you get time.

- [The Elements of Statistical Learning - Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)
- [Machine Learning - Tom Mitchell](http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf) -- I think this wonderful textbook is under-appreciated.
- [Artificial Intelligence: A Modern Approach - Russel & Norvig](http://web.cecs.pdx.edu/~mperkows/CLASS_479/2017_ZZ_00/02__GOOD_Russel=Norvig=Artificial%20Intelligence%20A%20Modern%20Approach%20(3rd%20Edition).pdf)
- [mlcourse.ai](https://mlcourse.ai/) -- Of all the introductions to machine learning I think this is the one that strikes the best balance between theory and practice.
- [Machine learning cheat sheets - Shervine Amidi](https://stanford.edu/~shervine/teaching/cs-229.html)
- [Kalman and Bayesian Filters in Python - Roger Labbe](http://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb) -- Kalman filters are notoriously hard to grok, this tutorial nicely builds up the steps to understanding them.
- [CS231n Convolutional Neural Networks for Visual Recognition - Stanford](http://cs231n.github.io/convolutional-networks/)
- [Algorithmes d’optimisation non-linéaire sans contrainte (French) - Michel Bergmann](https://www.math.u-bordeaux.fr/~mbergman/PDF/These/annexeC.pdf)
- [Graphical Models in a Nutshell - Koller et al.](https://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf)
- [Rules of Machine Learning: Best Practices for ML Engineering - Martin Zinkevich](https://developers.google.com/machine-learning/guides/rules-of-ml/) -- You should read this once a year.
- [A Few Useful Things to Know about Machine Learning - Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) -- This short paper summarizes basic truths in machine learning.
- [Choose Boring Technology - Dan McKinley](http://mcfunley.com/choose-boring-technology)
- [How to Write a Spelling Corrector - Peter Norvig](https://norvig.com/spell-correct.html) -- Magic in 36 lines of code.
- [MCMC sampling for dummies - Thomas Wiecki](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)
- [Your Easy Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)
- [An Intuitive Explanation of Convolutional Neural Networks - Ujjwal Karn](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)
- [An overview of gradient descent optimization algorithms - Sebastian Ruder](http://ruder.io/optimizing-gradient-descent/)
- [How to explain gradient boosting - Terence Parr and Jeremy Howard](https://explained.ai/gradient-boosting/index.html) -- A very good introduction to vanilla gradient boosting with step by step examples.
- [Why Does XGBoost Win "Every" Machine Learning Competition? - Didrik Nielsen](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf) -- This Master's thesis goes into some of the details of XGBoost without being too bloated.
- [Good sleep, good learning, good life - Piotr Wozniak](https://web.archive.org/web/20181017190008/https://www.supermemo.com/en/articles/sleep) -- Extremely long and nothing to do with data science, but a very thorough essay nonetheless on how to properly sleep.
- [Make for data scientists - Paul Butler](http://blog.kaggle.com/2012/10/15/make-for-data-scientists/) -- I believe Makefiles are yet to be rediscovered for managing data science pipelines.
- [Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations](https://fermatslibrary.com/s/statistical-tests-p-values-confidence-intervals-and-power-a-guide-to-misinterpretations) -- Just read it.
- [The Cramér-Rao Lower Bound on Variance: Adam and Eve’s "Uncertainty Principle" - Michael Powers](https://web.archive.org/web/20100613220918/http://astro.temple.edu/~powersmr/vol7no3.pdf)
- [Kaggle contest on Observing Dark World - Cam Davidson-Pilon](https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter5_LossFunctions/Ch5_LossFunctions_PyMC3.ipynb#Example:-Kaggle-contest-on-Observing-Dark-World) -- If you're not convinced about the power of Bayesian machine learning then read this and get your mind blown.
- [A Concrete Introduction to Probability (using Python) - Peter Norvig](https://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb) -- Extremely elegant Python coding.
- [The Hungarian Maximum Likelihood Trick - Louis Abraham](https://louisabraham.github.io/notebooks/hungarian_trick.html)


## Blogs

This is a list of blogs I regularly scroll through.

- [Tim Salimans on Data Analysis](http://timsalimans.com/)
- [Randal Olson](http://www.randalolson.com/blog/)
- [Sam & Max](http://sametmax.com/) -- French and NSFW!
- [Sebastian Raschka](http://sebastianraschka.com/blog/index.html)
- [Clean Coder](https://sites.google.com/site/unclebobconsultingllc/)
- [Pythonic Perambulations](https://jakevdp.github.io/)
- [Erik Bernhardsson](http://erikbern.com/)
- [otoro](http://blog.otoro.net/)
- [Terra Incognita](http://blog.christianperone.com/)
- [Real Python](https://realpython.com/blog/)
- [Airbnb Engineering](http://nerds.airbnb.com/)
- [No Free Hunch](http://blog.kaggle.com/)
- [The Unofficial Google Data Science Blog](http://www.unofficialgoogledatascience.com/)
- [will wolf](http://willwolf.io/)
- [Edwin Chen](http://blog.echen.me/)
- [Use the index, Luke!](http://use-the-index-luke.com/)
- [Jack Preston](https://unwttng.com/)
- [Agustinus Kristiadi](https://wiseodd.github.io/)
- [DataGenetics](http://datagenetics.com/blog.html)
- [Katherine Bailey](https://katbailey.github.io/)
- [Netflix Research](https://research.netflix.com/)
- [inFERENce](https://www.inference.vc/)
- [Hyndsight](https://robjhyndman.com/hyndsight/) -- Rob Hyndman is a time series specialist
- [While My MCMC Gently Samples](https://twiecki.io/)
- [Ines Montani](https://ines.io/) -- By one of the founders of [spaCy](https://spacy.io/)
- [Stephen Smerity](https://smerity.com/articles/articles.html)
- [Peter Norvig](http://norvig.com/)
- [IT Best Kept Secret Is Optimization](https://www.ibm.com/developerworks/community/blogs/jfp/?lang=en) -- By Jean-Francois Puget, aka [CPMP](https://www.kaggle.com/cpmpml)
- [explained.ai](https://explained.ai/)
- [Better Explained](https://betterexplained.com/)
- [Genetic Argonaut](https://geneticargonaut.blogspot.com/)
- [pandas blog](https://pandas-dev.github.io/pandas-blog/)
- [Towards Data Science](https://towardsdatascience.com/)
- [Linear Disgressions](https://lineardigressions.com/) -- data science podcasts
- [Probably Overthinking It](https://www.allendowney.com/blog/)
- [Simply Statistics](https://simplystatistics.org/)
- [Practically Predictable](http://practicallypredictable.com/)
- [koaning.io](http://koaning.io/) -- By Vincent Warmerdam, who did [this](https://www.youtube.com/watch?v=68ABAU_V8qI&feature=youtu.be) great presentation
